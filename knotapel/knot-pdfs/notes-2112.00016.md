# Paper: [Craven, Hughes, Jejjala, Kar 2022] "Learning knot invariants across dimensions"
**File:** 2112.00016v2.pdf
**arXiv:** 2112.00016v2, October 2022

## Paths Not Taken

- **Instanton Floer homology with SU(N) and level-rank duality** (p.14, Section 3.5): The instanton Floer homology used by Kronheimer-Mrowka [30] to construct s^sharp is naturally Z/4Z-graded, while Khovanov homology is fully Z-graded. The authors note "Perhaps more insight could be gained in this direction by considering instanton Floer homology with gauge group SU(N) and using level-rank duality [31]." This is a concrete unexplored avenue — lifting to higher-rank gauge groups to recover the full Z-grading.

- **Knot-quiver correspondence for organizing physical knot invariants** (p.31, Section 5): "The knot-quiver correspondence [52] may be helpful to more generally organize relationships between physical knot invariants." Mentioned only in passing, credited to Sergei Gukov for bringing it to their attention.

- **Spectral sequences relating Khovanov to instanton Floer** (p.30, Section 5): "Another approach which may be useful begins with the observation that there are spectral sequences relating Khovanov homology to e.g., instanton Floer homology [50, 51]. The gauge theory interpretation of these spectral sequences is unclear, but finding such an interpretation may help in relating J(q) to s." The gauge-theory *meaning* of these spectral sequences is an open question.

- **Deformation of supercharges via spectral sequences** (p.31, Section 5): Kh(q, -q^{-2}) specialization may encode information about deformations of the topological supercharge Q. They suggest the perspective on deformations of supercharges via spectral sequences in Gukov-Nawata-Saberi-Stosic-Sulkowski [53] "would be useful here" but don't pursue it. Witten himself pointed out to them that the 2D symmetry breaking pattern sl(2) -> sl(1) + sl(1) used to get Lee homology "may not be the relevant mechanism in five dimensions."

- **Neural network scan of all invariant pairs** (p.30, Section 5): "One could easily imagine using neural networks to scan through all pairs of invariants and determine which can be predicted from knowledge of the other." They propose but don't execute a systematic all-pairs correlation scan across the KnotInfo database. This would be a comprehensive map of which invariants "know about" which others.

- **Generating knots with |s| < 2g** (p.29, fn.29): "A useful step forward would be to find a knot generation procedure which yields many knots for which |s| < 2g." Their dataset was overwhelmingly saturated (|s| = 2g), which limited their ability to distinguish whether the network learns s or g independently.

## Technical Machinery

- **Lee spectral sequence filtration mechanics** (p.7-9, Section 2.2-2.3): The proof that |s| <= 2g works by decomposing a cobordism Sigma into elementary pieces via Morse theory on the projection map. Each elementary cobordism (Reidemeister move, cup/cap, saddle) induces a filtered map on Lee homology of specific filtration degree (0, +1, or -1). The total cobordism's filtration degree equals chi(Sigma) = v - e, where v = local maxima+minima and e = saddle points. This gives the Euler characteristic bound. The key insight: saddle surgeries are the *only* moves that lower filtration — so the slice genus is literally counting "how many saddles you need."

- **The t -> tq^{-2} substitution trick** (p.22-24, Section 4.3.1): A clever two-stage decomposition of the Kh(q, -q^{-2}) specialization. First substitute t -> tq^{-2} which "flattens" the diagonal knight-move structure of Khovanov homology into horizontal rows. Then set t = -1. For homologically thin knots, this collapses knight-move pairs (which cancel by alternating sign) leaving only the lone pawn move, producing a polynomial whose terms are completely determined by s and the homological width. This is a new way to understand *why* certain polynomial specializations encode s.

- **Khovanov polynomial factored via knight moves** (p.7, eq.2.9): The factorization Kh(K;q,t) = q^s(q + q^{-1}) + sum_{l>=1} f_{2l}(q,t)(1 + tq^{4l}) neatly separates the "pawn move" (which carries s) from all "knight moves." The knight move conjecture posits f_{2l} = 0 for l > 1, now known to be false (Manolescu-Marengon counterexample at 38 crossings).

- **s-invariant as large-k limit** (p.14, eq.3.8): The formula lim_{k->infinity} k log(Z_5[K;k] / Z_5[unknot;k]) = 2*pi*i*s provides a 5d gauge theory "trace formula" for the Rasmussen invariant, connecting it to the growth rate of the graded trace in the Hilbert space of a 5d SYM theory. The saddle-point approximation gives Z_5[k] ~ e^{2*pi*i*s/k + log 2}, reminiscent of the volume conjecture but for s rather than hyperbolic volume.

## Speculative Remarks

- **"What is four-dimensional about the Jones polynomial?"** (p.4, p.29-30, Section 5): This is the paper's central mystery. The Jones polynomial J(q) is defined 2-dimensionally (Hecke algebra trace) and has a 3d interpretation (Chern-Simons). But the neural network results show J(q) predicts 4d invariants (s, g) with >95% accuracy. The authors emphasize: "there is no obvious route by which the Jones polynomial may be manipulated into revealing the s-invariant." They speculate the explanation may involve analytic continuation of the Chern-Simons path integral (citing Witten's prior work on analytic continuation explaining the volume conjecture).

- **Chern-Simons knows more than it should** (p.30, Section 5): "These results could imply that Chern-Simons gauge theory knows a little bit more about the five-dimensional gauge theory Hilbert space than is naively expected." In a prior case where CS appeared to contain "too much" information, the explanation was analytic continuation of the path integral [14]. They suggest the same mechanism may be at work here.

- **s may be extractable from Khovanov homology via a non-Lee route** (p.24-25, p.30, Section 4.3.1 & 5): The observation that Kh(q, -q^{-2}) encodes s with coefficients seemingly determined by s (not just the power offset) "suggests that perhaps s is encoded in some additional way within Khovanov homology, one which does not make reference to Lee homology or the quantum grading." This is a hint at a second, unknown extraction path for s from Kh.

- **Kh(q, -q^{-2}) as deformation of topological supercharge** (p.31, Section 5): The specialization at t = -q^{-2} "may be interpreted as a statement about the existence of deformations of the topological supercharge Q." The nontrivial prediction from Lee's theorem is that the cohomology of the deformed supercharge is 2-dimensional, and the Kh(q, -q^{-2}) results "suggest that correlating the fermion number grading with the instanton number grading in this way produces, e.g., a number of ground states with instanton numbers that depend on s."

- **Machine learning for SPC4** (p.31, Section 5): They propose using neural networks to identify slice knots from braid words or other invariants, specifically to help construct counterexamples to the smooth 4-dimensional Poincare conjecture. Since detecting g = 0 vs g != 0 is simpler than learning g, and their network achieves 98.61% accuracy on this binary task, this is a concrete ML application to a major open problem.

## DKC Overlaps

- **Jones polynomial at roots of unity predicting 4d invariants** (p.25-26, Table 6, Section 4.4): They evaluate J(q) at roots of unity e^{pi*i*n/(k+2)} for k in [3,10] and find >96% accuracy for predicting both s and g across ALL these roots. The accuracy is remarkably stable across different k values (varying only between 93% and 97% for s). **This is directly relevant to DKC**: it shows that evaluations at *specific* roots of unity carry essentially the same 4d information. The fact that small k (k=3,4,5) works almost as well as large k connects to DKC's interest in what happens at small roots — particularly the quantum dimension singularity question.

- **The n=0 case: Jones polynomial itself** (p.20-21, Table 1): When n=0, Kh(q, -q^0) = Kh(q, -1) which is just J(q). Even the Jones polynomial alone predicts s with ~94.8% and g with ~97.2% accuracy. **DKC relevance**: The raw polynomial (before any root-of-unity evaluation) already "knows" about 4d topology. This supports the DKC finding that bracket values carry computational information — the information is in the polynomial structure itself, not just at special evaluation points.

- **Writhe/normalization sensitivity** (p.25, Section 4.4): The neural network is learning an *association*, not a function — "there are knots with the same Jones polynomial but different slice genus and s-invariant" (fn.28: knots 5_1 and 10_132 are such a pair). **DKC relevance**: This means the Jones polynomial (which normalizes away writhe) loses information that the raw bracket retains. This is consistent with DKC's emphasis on using the *unnormalized* bracket because writhe carries computational information.

- **Quantum grading centered on s** (p.22-24, Section 4.3.1): After the t -> tq^{-2} substitution, the powers of q in the resulting polynomial cluster around s. The s-invariant literally determines *where* the Khovanov homology sits in quantum grading space. **DKC relevance**: In DKC, the axiality theorem says the bracket at delta=0 lives on a single Z[zeta_8] axis. The Khovanov centering around s is a categorified analog — the "address" of the homology in grading space is itself a computational invariant.

- **Simple two-layer networks suffice** (p.16-17, Section 4): A two-hidden-layer network with 100 nodes each (using ReLU activation) achieves >95% accuracy. The results are "stable under adjustments to the neural network architecture" including adding layers, neurons, dropout, or different activation functions. **DKC relevance**: This suggests the underlying relationship is relatively simple / low-dimensional, consistent with DKC's finding that small discrete structures (e.g., 24-element binary octahedral group) can capture the essential computational content.

- **Instanton number = quantum grading in 5d** (p.12, Section 3.3): In the 5d formulation, "the object we call instanton number is really an operator acting on the Hilbert space." The quantum grading of Khovanov homology arises from this instanton number operator. **DKC relevance**: This gives a physical interpretation to the grading structures that DKC exploits — the "crossing depth" in DKC (more generator multiplications = more computational power) may be related to instanton number in the gauge theory picture.

- **Knight move conjecture failure at 38 crossings** (p.7-8, p.15): The counterexample involves "pairs of additional supersymmetric solutions in the path integral which have quantum numbers that differ by more than one in fermion number or more than four in instanton number." **DKC relevance**: The failure of the knight move conjecture at high crossing number means Khovanov homology has more complex structure than the simple pawn+knight pattern for complicated knots. This is relevant to DKC's crossing depth hierarchy — more crossings = richer structure, eventually breaking the simple pattern.

## Surprises

- **Kh(q, -q^{-2}) works BETTER than Kh(q, -q^{-4}) for predicting s** (p.20-21, Table 1): The n = -2 case gives 99.88% accuracy for s, compared to 99.77% for n = -4. This is surprising because n = -4 is the one predicted by the knight move conjecture / gauge theory (eq.3.7). The n = -2 specialization has no known theoretical explanation — it's a "novel relationship between the Khovanov and Lee homology theories of a knot" (p.4). The authors call this "more surprising" than the n = -4 result.

- **Jones polynomial determines signature mod 4** (p.18, fn.25): "It is a folklore result that the Jones polynomial determines the signature mod 4." This is surprisingly little — the signature is a fundamental classical invariant, and the Jones polynomial can only pin it down mod 4. Yet the neural network predicts s (which equals the signature 96.5% of the time) with >95% accuracy from J(q). The network is extracting more than the mod-4 information.

- **The 5d theory is not UV-complete** (p.13, fn.5): The gauge theory construction that gives Khovanov homology uses a 5d maximal SYM theory that "is not ultraviolet complete." This means the construction is inherently an approximation — the "true" theory lives in 6 dimensions ((0,2) theory), which has no Lagrangian description. Building fundamental math invariants on a non-UV-complete theory is unusual.

- **Witten's construction is not proven to equal Khovanov homology** (p.10, fn.12): "It is not actually proven in [7] that the construction described there coincides with Khovanov homology." The entire gauge theory framework for understanding Khovanov homology rests on an unproven identification. The paper still finds it "enough for us" since it produces a Floer-like homology with the Jones polynomial as its graded Euler characteristic.

- **s^sharp != s even for the trefoil** (p.14, Section 3.5): Kronheimer-Mrowka's instanton Floer invariant s^sharp differs from Rasmussen's s already for the simplest nontrivial knot (the trefoil). Despite both providing lower bounds on 2g, they are genuinely different invariants. This means the "natural" gauge theory construction gives a *different* s-like invariant, and the actual Rasmussen s requires something more specific (Lee deformation, which has no known 5d mechanism).

- **Multi-task learning shows no preference** (p.28-29, Section 4.5.1): Weighting the s-invariant loss 10x higher or 10x lower relative to the slice genus loss makes essentially no difference to either prediction accuracy. This is unexpected — if the network learned one through the other, you'd expect the weighting to matter. The invariants appear to be learned semi-independently despite their close relationship.
